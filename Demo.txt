def validate_llm_output(obj):
    """Validate JSON returned by Azure LLM."""
    if not isinstance(obj, dict):
        return False, "Top-level structure must be a JSON object"

    required_top = ["Identifier", "date", "from", "to", "subject", "body", "categories"]
    for f in required_top:
        if f not in obj:
            return False, f"Missing required field: {f}"

    if not isinstance(obj["categories"], list):
        return False, "categories must be a list"

    for c in obj["categories"]:
        if "category" not in c or "reason" not in c:
            return False, "Each category must contain category + reason"

        # Accept BOTH formats
        if "sourcelines" not in c:
            # Convert sourceline_quotes → sourcelines
            if "sourceline_quotes" in c:
                c["sourcelines"] = [{"lines": x, "feedback": ""} for x in c["sourceline_quotes"]]
                del c["sourceline_quotes"]
            else:
                return False, "Missing sourcelines array"

        # Ensure sourcelines are well-formed
        for s in c["sourcelines"]:
            if "lines" not in s:
                return False, "Each sourceline must have lines field"
            if "feedback" not in s:
                s["feedback"] = ""

    return True, "OK"

===========================================================================

LLM_PROMPT_TEMPLATE = (
    "You are a strict compliance classifier. Return ONLY a JSON object.\n"
    "NO explanation, NO commentary, NO backticks.\n"
    "Your output MUST follow EXACTLY this structure:\n\n"
    "{\n"
    "  \"Identifier\": \"<id>\",\n"
    "  \"date\": \"<string>\",\n"
    "  \"from\": \"<string>\",\n"
    "  \"to\": \"<string>\",\n"
    "  \"subject\": \"<string>\",\n"
    "  \"body\": \"<string>\",\n"
    "  \"categories\": [\n"
    "    {\n"
    "      \"category\": \"<one of: {categories_list}>\",\n"
    "      \"sourcelines\": [\n"
    "         {\"lines\": \"<text excerpt>\", \"feedback\": \"\"}\n"
    "      ],\n"
    "      \"reason\": \"<why this category applies>\"\n"
    "    }\n"
    "  ],\n"
    "  \"priority\": <0-10>,\n"
    "  \"falsePositive\": false,\n"
    "  \"manualOverride\": false,\n"
    "  \"raiseAlarm\": false\n"
    "}\n\n"
    "Email content to classify:\n"
    "{email_text}\n"
)

=========================================================

def call_azure_llm(client, masked_email, categories_list):
    """Call Azure OpenAI with guaranteed strict JSON return."""
    message = [
        {"role": "system", "content": LLM_PROMPT_SYSTEM},
        {
            "role": "user",
            "content": LLM_PROMPT_TEMPLATE.format(
                email_text=json.dumps(masked_email, indent=2),
                categories_list=",".join(categories_list)
            )
        }
    ]

    try:
        response = client.chat.completions.create(
            model=os.getenv("AZURE_DEPLOYMENT"),
            messages=message,
            temperature=0,
            response_format={"type": "json_object"}
        )
    except Exception as e:
        # THIS is the line that helps us avoid silent fallback
        raise RuntimeError(f"Azure LLM communication failed: {e}")

    try:
        parsed = json.loads(response.choices[0].message.content)
        return parsed
    except Exception as e:
        raise RuntimeError(f"Azure returned invalid JSON: {e}")

=================================================≈==========================

def classify_email_with_validation(client, masked_email, categories_list, ident):
    if client is None:
        simulate_db_log_action(ident, "NO_CLIENT", {})
        return conservative_rules(masked_email)

    try:
        parsed = call_azure_llm(client, masked_email, categories_list)
        ok, msg = validate_llm_output(parsed)
        if not ok:
            simulate_db_log_action(ident, "LLM_INVALID", {"reason": msg})
            # DO NOT FALLBACK SILENTLY – send back structure with error
            raise RuntimeError(f"LLM returned invalid structure: {msg}")
        return parsed

    except Exception as e:
        # Visual error -- user SEES the real problem
        st.error(f"LLM ERROR in ID {ident}: {e}")
        simulate_db_log_action(ident, "LLM_FAIL", {"error": str(e)})
        return conservative_rules(masked_email)

======================================
# Replace your broken prompt block with this exact variable.
# When you format it, call: prompt = LLM_PROMPT_TEMPLATE.format(subject=..., body=..., categories_list=...)

LLM_PROMPT_TEMPLATE = """You are a strict email compliance classifier. Return ONLY a JSON object and NOTHING ELSE.

Given the email below, do the following:
subject: {subject}
body: {body}

NO explanation, NO commentary, NO backticks.

Determine which of the following categories this email most likely belongs to (an email can belong to multiple categories).
Allowed potential categories are: secrecy, Market Manipulation/Misconduct, Market Bribery, Change in communication style

Return only valid JSON in the exact format below (no extra text).
Your output MUST follow EXACTLY this structure:

{{
  "Identifier": "<id>",
  "date": "<string>",
  "from": "<string>",
  "to": "<string>",
  "subject": "<string>",
  "body": "<string>",
  "categories": [
    {{
      "category": "<one of: {categories_list}>",
      "sourcelines": [
        {{
          "lines": "<text excerpt>",
          "feedback": ""
        }}
      ],
      "reason": "<why this category applies>"
    }}
  ],
  "priority": <0-10>,
  "FalsePositive": false,
  "manualoverride": false,
  "raiseAlarm": false
}}

IMPORTANT:
- If the email is compliant (no relevant categories), return "categories": [] (an empty list).
- The "priority" must be a single integer from 0 to 10.
- Do NOT return any other keys, commentary, or text outside the JSON.
"""
======================================
def compute_priority_explainable(email_body: str, categories: List[Dict[str, Any]]) -> int:
    if not categories:
        return 1  # compliant email
    
    CATEGORY_SEVERITY = {
        "Market Bribery": 5,
        "Market Manipulation/Misconduct": 4,
        "secrecy": 3,
        "Change in communication style": 2,
        "Employee ethics": 2,
        "complaints": 1
    }

    # 1. Base severity = maximum severity of detected categories
    base = max(CATEGORY_SEVERITY.get(c["category"], 0) for c in categories)

    # 2. Evidence strength via number of sourcelines
    findings_count = sum(len(c.get("sourcelines", [])) for c in categories)
    if findings_count <= 1:
        evidence = 0
    elif findings_count == 2:
        evidence = 1
    elif findings_count == 3:
        evidence = 2
    else:
        evidence = 3

    # 3. Volume score = number of distinct categories
    distinct = len({c["category"] for c in categories})
    if distinct == 1:
        volume = 0
    elif distinct == 2:
        volume = 1
    else:
        volume = 2

    # 4. Cross-risk escalation (+1 max)
    names = {c["category"] for c in categories}
    escalation = 0
    critical_pairs = [
        ("secrecy", "Market Manipulation/Misconduct"),
        ("secrecy", "Market Bribery"),
        ("Market Manipulation/Misconduct", "Market Bribery"),
        ("complaints", "Employee ethics"),
    ]
    for a, b in critical_pairs:
        if a in names and b in names:
            escalation = 1
            break

    priority_raw = base + evidence + volume + escalation
    return max(1, min(priority_raw, 10))

''' priority_raw = base + evidence + volume + escalation
priority = min(max(priority_raw, 1), 10)

#Example - 
Case: secrecy + market manipulation
2 sourcelines
2 categories

Base severity = 4
Evidence = 1 (2 findings)
Volume = 1 (two categories)
Escalation = 1 (critical pair)
Total = 4 + 1 + 1 + 1 = 7


Severity	Risk category severity	1–5
Evidence	How many matched patterns(sourcelines)	0–3
Volume	    How many categories triggered	0–2
Escalation	Known risky combinations	0–1

Final priority clipped to 1–10.
'''

